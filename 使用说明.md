

# 实现代码说明（执行环境、运行流程和对应算法）

## 1. 执行环境 (Execution Environment)

本项目基于 Windows 操作系统进行开发与测试，利用 Python 串联数据预处理、语义分割与语义融合流程，并结合 COLMAP 软件进行三维稀疏重建。

**环境与依赖：**

* **Python版本**：Python 3.10
* **核心依赖**：

包与Deeplab V3+一致   
colmap使用的是课上提供的版本


---

## 2. 运行流程 

整个系统分为四个串行模块，流程如下：

**第一阶段：数据采集与预处理**

1. 使用移动设备围绕目标建筑拍摄一段视频（约3分钟），确保包含建筑主体、地面、植被及移动行人。
2. 运行 `video_to_images.py` 脚本，以固定帧间隔（如每 35 帧）截取图像，生成图像序列数据集。此步骤确保了相邻图像间具有足够的视差以满足 SfM 重建要求。

**第二阶段：2D 语义分割**

1. 运行dpl目录下的，`run_for_hw.py` 脚本对输入图像序列进行批量推理，生成对应的语义掩膜（Mask）。
2. 输出的掩膜为单通道灰度图，像素值对应语义类别 ID（如 2 代表建筑，8 代表植被，0 代表地面），作为后续融合的语义源。
3. 输入为从视频截取的图像，输出为一个包含大量 `.jpg` 图片的文件夹，文件内容为按顺序编号的图片（例如 `frame_00000.jpg`, `frame_00001.jpg`...）。

**第三阶段：稀疏点云重建**

1. 使用 COLMAP 软件导入Mask之后的图像序列，文件夹为Masks/Images_For_Colmap_1
2. 执行特征提取（Feature Extraction）与特征匹配（Feature Matching）。
3. 运行重建（Reconstruction），生成场景的稀疏点云及相机位姿参数。
4. 导出重建模型为文本格式（`images.txt`, `points3D.txt`, `cameras.txt`）。

**第四阶段：语义融合与上色**

1. 运行 `semantic_fusion.py` 脚本，读取 COLMAP 导出的 3D 点云数据与 2D 语义掩膜。

   Mask一般在./Masks/Mask1 

   model导入从./models

2. 通过重投影机制建立 2D-3D 对应关系。

3. 根据投票策略修改 3D 点的颜色属性（RGB），实现建筑（红）、植被（绿）、地面（灰）的语义可视化，并对动态物体（人/车）进行颜色剔除（置黑）。

---

## 3. 对应算法 

本项目核心涉及三种算法逻辑：

### (1) 语义分割算法：DeepLabV3+

为了获取高质量的 2D 语义标签，采用了 DeepLabV3+ 网络架构。

* **空洞空间金字塔池化**：通过不同扩张率的空洞卷积，在不降低分辨率的情况下扩大感受野，从而有效捕获多尺度的上下文信息（如区分大面积的地面与小尺度的行人）。
* **编码器-解码器结构**：编码器提取高层语义特征，解码器将低层细节特征与高层特征融合，并进行上采样恢复物体边缘细节，确保生成的 Mask 边缘贴合建筑物轮廓。

### (2) 三维重建算法：运动恢复结构 (SfM)

COLMAP 采用了增量式 SfM 算法流程：

* **特征提取**：使用 SIFT 算法在每张图像中检测关键点，该算法对旋转、缩放和亮度变化具有鲁棒性。
* **特征匹配**：通过极线几何约束在不同视角的图像间寻找同名点。
* **三角测量与光束法平差 ***：利用三角测量计算特征点的三维空间坐标，并通过非线性优化方法（光束法平差）同时优化相机位姿参数与三维点坐标，最小化重投影误差。

### (3) 语义融合算法：多视图投票机制 

这是实现语义上色的核心算法。由于语义分割在单张图像上可能存在噪声，本作业采用多视图一致性校验来提高准确率。

**算法步骤：**

1. **重投影**：对于每一个 3D 点 ，根据 `images.txt` 中记录的观测关系，找到所有能观测到该点的图像 。利用该图像的相机位姿和内参，找到其在 2D 图像上的投影像素坐标 。
2. **语义采样**：读取图像  对应的语义掩膜 ，获取坐标  处的类别标签 。
3. **投票统计**：该 3D 点会收到来自各个视角的语义投票 。算法统计其中出现频率最高的标签作为该点的最终语义类别。
* 公式：


4. **颜色映射与过滤**：根据查表赋予颜色。若最终判定为动态物体（如行人 ID=11），则强制设为黑色或剔除，从而实现静态场景的纯净重建。